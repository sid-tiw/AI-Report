\documentclass[journal, compsoc]{IEEEtran}

\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage[euler-digits,euler-hat-accent]{eulervm}

\bibliographystyle{IEEEtran}

\newcommand\Mycomb[2][^n]{\prescript{#1\mkern-0.5mu}{}C_{#2}}

\title{Artificial Intelligence Report on Solutions to Laboratory Problems}

\author{Siddhartha~Tiwari, Siddharth~Mani~Tiwari, Saurabh~Kumar, Pushkar~Tiwari}

%\author{
%\IEEEauthorblockN{Siddhartha Tiwari}\IEEEauthorblockA{201851127}
%\and
%\IEEEauthorblockN{Siddharth Mani Tiwari}\IEEEauthorblockA{201851126}
%\and
%\IEEEauthorblockN{Saurabh Kumar}\IEEEauthorblockA{201851113}
%\and
%\IEEEauthorblockN{Pushkar Tiwari}\IEEEauthorblockA{201851095}
%}

\begin{document}
\IEEEtitleabstractindextext{%
\begin{abstract}
This report is the part two of discussions on laboratory problems assigned to us by Dr. Pratik Shah. The problems discussed are from various fields including Hidden Markov Models, Markov Random Fields, Hopfield Networks, Unsupervised Learning, N-Arm Bandit e.t.c. For each problem, the most efficient solution is arrived upon gradually, using different techniques taught in the class.
\end{abstract}
}
\maketitle

\section{Hidden Markov Models and Expectation Maximization}
\subsection{Expectation Maximization for finding the Bias of bent coins}
\subsubsection{Introduction}
This problem is to find out the unknown biases of bent coins using the results of random experiments done on the coins.
We can "guess" the biases of coins smartly using Expectation Maximization, which uses the maximum likelihood along with some guesses
to find a more "educated" guess. It uses recursion to improve the guesses. The algorithm is explained in the next subsection for $10$ biased coins.
\subsubsection{The Expectation Maximization algorithm}
In this algorithm we have to start with an intial guess of the biases of the coins. Let the biases be $\hat{p}_{1}$, $\hat{p}_{2} \ldots$, $\hat{p}_{10}$ for the $10$
bent coins given in the problem. These biases represent the probability to obtain $1$ (heads) when the coins are tossed. In each iteration the algorithm
have two steps, the E-step and the M-step.

In the E-step, we calculate that what the probability is, that a certain coin was picked, given the obeservation sequence. This probabilities can be
calculated easily using Bayes theorem. Assume that $O$ represents the observation sequence and the coins are numbered from $1$ to $10$, then the
probability that coin $1$ was used to get $O$ is given by:
\begin{equation}
\label{eqn:bayes}
    P(C = 1 | O) = \frac{P(O | C = 1) \cdot P(C = 1)}{P(O)}
\end{equation}
In the equation \ref{eqn:bayes}, we have used the Bayes theorem to obtain the probability in terms of likelihood, prior and evidence. Now,
since all the coins are equally likely to choose, ${P(C = 1) = \frac{1}{10}}$. Also, P(O) can be rewritten by marginalizing on all the coins:
\begin{equation}
\label{eqn:marginalization}
\begin{aligned}
P(O) &= \sum_{C = 1}^{10} P(C, O) = \sum_{C = 1}^{10} P(O | C) \cdot P(C)\\
&= \frac{1}{10} \sum_{C = 1}^{10} P(O | C)
\end{aligned}
\end{equation}
because, ${P(C) = \frac{1}{10} \hspace{1mm} \forall \hspace{1mm} C \in \{1, 2, 3 \ldots 10\}}$.

So, using the result obtained in the equation \ref{eqn:marginalization}, equation \ref{eqn:bayes} can be rewritten as:
\begin{equation}
\label{eqn:final}
\begin{aligned}
P(C = 1 | O) &= \frac{P(O | C = 1) \cdot \frac{1}{10}}{\frac{1}{10}\sum_{C = 1}^{10} P(O | C)}\\
&= \frac{P(O | C = 1)}{\sum_{C = 1}^{10} P(O | C)}
\end{aligned}
\end{equation}

Now, assume that $n$ and $m$ are two parameters that represent number of heads and number of tails respectively, in $O$. Since, the trials are mutually
independent, the probability $P(C = 1 | O)$ is given by:
\begin{equation}
\label{eqn:finale}
\begin{aligned}
P(C = 1 | O) &= \frac{P(O | C = 1)}{\sum_{C = 1}^{10} P(O | C)}\\
&= \frac{\hat{p}_{1}^{n}\cdot (1 - \hat{p}_{1})^{m}}{\sum_{i = 1}^{10} \hat{p}_{i}^{n}\cdot (1 - \hat{p}_{i})^{m}}
\end{aligned}
\end{equation}

All the other probabilities can be calculated similarly. Now, expected number of heads for the coin $x$ in $O$ will be:
\begin{equation}
\label{eqn:heads}
\begin{aligned}
\mathbb{E}[N_{H}] &= n \cdot P(C = x | O) \hspace{2mm}\\
&= \frac{n\cdot \hat{p}_{x}^{n}\cdot (1 - \hat{p}_{x})^{m}}{\sum_{i = 1}^{10} \hat{p}_{i}^{n}\cdot (1 - \hat{p}_{i})^{m}}
\end{aligned}
\end{equation}

We can also obtain the expected number of tails in the same fashion.

So, in the E-step we find out the expected number of heads and tails obtained by using the parameters of that particular iteration.

In the M-step we use the expected number of heads and tails obtained in the E-step to get a new estimation of the biases of all the coins.

In Expectation Maximization these two steps are repeated until the value of parameters converge to some certain value. The biases thus obtained
are the best guesses Expectation Maximization can obtain.

We have coded this expectation maximization in Python(Jupyter Notebook). Also, we noted that if we keep the biases of two different coins the same
at the start the biases obtained after the expectation maximization are also same for the respective coins.
Hence, assuming that every coin has a different bias (initializing the coins with different biases),
the biases obtained are given in the table \ref{table:biases}.
\begin{table}[!h]
\renewcommand{\arraystretch}{0.4}
\caption{Biases of the coins}
\label{table:biases}
\centering
\begin{tabular}{|c|c|c|}
\hline
{\bfseries Coin} & {\bfseries Initial Biases} & {\bfseries Final Biases}\\
\hline\hline
1 & 0.05 & 0.010000\\
\hline
2 & 0.15 & 0.092424\\
\hline
3 & 0.25 & 0.176789\\
\hline
4 & 0.35 & 0.287559\\
\hline
5 & 0.45 & 0.391205\\
\hline
6 & 0.55 & 0.477410\\
\hline
7 & 0.65 & 0.569790\\
\hline
8 & 0.75 & 0.688256\\
\hline
9 & 0.85 & 0.784089\\
\hline
10 & 0.95 & 0.897061\\
\hline
\end{tabular}
\end{table}

\nocite{Do2008}
\bibliography{references}
\end{document}
